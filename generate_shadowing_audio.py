"""
Gemini Live APIë¥¼ ì‚¬ìš©í•œ Shadowing ì˜¤ë””ì˜¤ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ (v2)

ê° ë¬¸ì¥ì„ ê°œë³„ ìƒì„± í›„ ê³µë°±ì„ ì‚½ì…í•˜ì—¬ í•©ì¹©ë‹ˆë‹¤.

## ì‚¬ìš©ë²•
python generate_shadowing_audio.py

## í•„ìš” íŒ¨í‚¤ì§€
pip install google-genai pydub
"""

import os
import asyncio
import wave
import subprocess
from pathlib import Path
import struct

from google import genai
from google.genai import types

# ì˜¤ë””ì˜¤ ì„¤ì •
RECEIVE_SAMPLE_RATE = 24000
CHANNELS = 1
SAMPLE_WIDTH = 2  # 16-bit audio = 2 bytes
SILENCE_DURATION = 4.0  # ë¬¸ì¥ ì‚¬ì´ ê³µë°± (ì´ˆ)

# ëª¨ë¸ ì„¤ì •
MODEL = "models/gemini-2.5-flash-native-audio-preview-12-2025"

# API í´ë¼ì´ì–¸íŠ¸
client = genai.Client(
    http_options={"api_version": "v1beta"},
    api_key=os.environ.get("GEMINI_API_KEY"),
)

# Live API ì„¤ì •
CONFIG = types.LiveConnectConfig(
    response_modalities=["AUDIO"],
    speech_config=types.SpeechConfig(
        voice_config=types.VoiceConfig(
            prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name="Zephyr")
        )
    ),
)

# Day 4 Shadowing ë¬¸ì¥ë“¤
SHADOWING_SENTENCES = [
    "Hello! My name is James.",
    "I am 32 years old.",
    "I am a software engineer.",
    "I work at a tech company in Seoul.",
    "I am married.",
    "My wife's name is Yuna.",
    "She is a teacher.",
    "We have one daughter.",
    "My family is small but happy.",
    "I love my life in Seoul!"
]


async def generate_single_sentence(text: str, output_path: str) -> bool:
    """ë‹¨ì¼ ë¬¸ì¥ì„ ì˜¤ë””ì˜¤ë¡œ ë³€í™˜í•˜ì—¬ WAV ì €ì¥"""
    print(f"  ğŸ¤ ìƒì„± ì¤‘: {text[:30]}...")
    
    audio_chunks = []
    
    try:
        async with client.aio.live.connect(model=MODEL, config=CONFIG) as session:
            await session.send(
                input=f"Read this sentence naturally in a warm, conversational tone: {text}",
                end_of_turn=True
            )
            
            turn = session.receive()
            async for response in turn:
                if data := response.data:
                    audio_chunks.append(data)
        
        if not audio_chunks:
            print(f"    âŒ ì˜¤ë””ì˜¤ ë°ì´í„° ì—†ìŒ")
            return False
        
        # PCM ë°ì´í„° ê²°í•© ë° WAV ì €ì¥
        pcm_data = b"".join(audio_chunks)
        with wave.open(output_path, "wb") as wav_file:
            wav_file.setnchannels(CHANNELS)
            wav_file.setsampwidth(SAMPLE_WIDTH)
            wav_file.setframerate(RECEIVE_SAMPLE_RATE)
            wav_file.writeframes(pcm_data)
        
        return True
    except Exception as e:
        print(f"    âŒ ì˜¤ë¥˜: {e}")
        return False


def create_silence_wav(duration: float, output_path: str):
    """ì§€ì •ëœ ê¸¸ì´ì˜ ë¬´ìŒ WAV íŒŒì¼ ìƒì„±"""
    num_samples = int(RECEIVE_SAMPLE_RATE * duration)
    silence_data = struct.pack('<' + 'h' * num_samples, *([0] * num_samples))
    
    with wave.open(output_path, "wb") as wav_file:
        wav_file.setnchannels(CHANNELS)
        wav_file.setsampwidth(SAMPLE_WIDTH)
        wav_file.setframerate(RECEIVE_SAMPLE_RATE)
        wav_file.writeframes(silence_data)


def combine_wav_files(wav_files: list, silence_path: str, output_path: str):
    """WAV íŒŒì¼ë“¤ì„ ê³µë°±ê³¼ í•¨ê»˜ í•©ì¹˜ê¸°"""
    print(f"\nğŸ“¦ ì˜¤ë””ì˜¤ íŒŒì¼ í•©ì¹˜ëŠ” ì¤‘...")
    
    combined_data = b""
    
    for i, wav_path in enumerate(wav_files):
        # ë¬¸ì¥ ì˜¤ë””ì˜¤ ì¶”ê°€
        with wave.open(wav_path, "rb") as wav_file:
            combined_data += wav_file.readframes(wav_file.getnframes())
        
        # ë§ˆì§€ë§‰ ë¬¸ì¥ì´ ì•„ë‹ˆë©´ ê³µë°± ì¶”ê°€
        if i < len(wav_files) - 1:
            with wave.open(silence_path, "rb") as silence_file:
                combined_data += silence_file.readframes(silence_file.getnframes())
    
    # í•©ì¹œ WAV ì €ì¥
    with wave.open(output_path, "wb") as output_wav:
        output_wav.setnchannels(CHANNELS)
        output_wav.setsampwidth(SAMPLE_WIDTH)
        output_wav.setframerate(RECEIVE_SAMPLE_RATE)
        output_wav.writeframes(combined_data)
    
    print(f"  âœ… WAV ì €ì¥ ì™„ë£Œ: {output_path}")


def convert_to_mp3(wav_path: str, mp3_path: str) -> bool:
    """WAVë¥¼ MP3ë¡œ ë³€í™˜"""
    try:
        subprocess.run([
            "ffmpeg", "-y", "-i", wav_path,
            "-codec:a", "libmp3lame", "-qscale:a", "2",
            mp3_path
        ], check=True, capture_output=True)
        print(f"  âœ… MP3 ë³€í™˜ ì™„ë£Œ: {mp3_path}")
        return True
    except subprocess.CalledProcessError as e:
        print(f"  âš ï¸ MP3 ë³€í™˜ ì‹¤íŒ¨: {e.stderr.decode()}")
        return False
    except FileNotFoundError:
        print("  âš ï¸ ffmpegê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return False


async def main():
    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    output_dir = Path("docs/assets/audio")
    temp_dir = output_dir / "temp_shadowing"
    output_dir.mkdir(parents=True, exist_ok=True)
    temp_dir.mkdir(parents=True, exist_ok=True)
    
    print("=" * 60)
    print("ğŸ¤ Day 4 Shadowing ì˜¤ë””ì˜¤ ìƒì„± (ê³µë°± ì‚½ì… ë²„ì „)")
    print("=" * 60)
    print(f"ğŸ“ ì´ {len(SHADOWING_SENTENCES)}ê°œ ë¬¸ì¥")
    print(f"â±ï¸ ë¬¸ì¥ ì‚¬ì´ ê³µë°±: {SILENCE_DURATION}ì´ˆ")
    print("=" * 60)
    
    # 1. ê° ë¬¸ì¥ ê°œë³„ ìƒì„±
    wav_files = []
    for i, sentence in enumerate(SHADOWING_SENTENCES, 1):
        wav_path = temp_dir / f"sentence_{i:02d}.wav"
        success = await generate_single_sentence(sentence, str(wav_path))
        if success:
            wav_files.append(str(wav_path))
            print(f"    âœ… ë¬¸ì¥ {i}/10 ì™„ë£Œ")
        else:
            print(f"    âŒ ë¬¸ì¥ {i}/10 ì‹¤íŒ¨")
        
        # API ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
        await asyncio.sleep(1)
    
    if len(wav_files) != len(SHADOWING_SENTENCES):
        print(f"\nâš ï¸ ì¼ë¶€ ë¬¸ì¥ ìƒì„± ì‹¤íŒ¨. {len(wav_files)}/{len(SHADOWING_SENTENCES)} ì™„ë£Œ")
    
    # 2. ê³µë°± WAV íŒŒì¼ ìƒì„±
    silence_path = temp_dir / "silence.wav"
    create_silence_wav(SILENCE_DURATION, str(silence_path))
    print(f"\nğŸ”‡ ê³µë°± íŒŒì¼ ìƒì„±: {SILENCE_DURATION}ì´ˆ")
    
    # 3. ëª¨ë“  íŒŒì¼ í•©ì¹˜ê¸°
    combined_wav = output_dir / "week1_day4_shadowing.wav"
    combine_wav_files(wav_files, str(silence_path), str(combined_wav))
    
    # 4. MP3ë¡œ ë³€í™˜
    final_mp3 = output_dir / "week1_day4_shadowing.mp3"
    if convert_to_mp3(str(combined_wav), str(final_mp3)):
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        import shutil
        shutil.rmtree(temp_dir)
        os.remove(combined_wav)
        print("\nğŸ§¹ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")
    
    print("\n" + "=" * 60)
    print("âœ… Shadowing ì˜¤ë””ì˜¤ ìƒì„± ì™„ë£Œ!")
    print(f"ğŸ“ íŒŒì¼ ìœ„ì¹˜: {final_mp3}")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
