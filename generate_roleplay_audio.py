"""
Gemini Live APIë¥¼ ì‚¬ìš©í•œ ì—­í• ê·¹(Role Play) ì˜¤ë””ì˜¤ ìƒì„± ìŠ¤í¬ë¦½íŠ¸

A/B ë‘ í™”ìë¥¼ ë‹¤ë¥¸ ëª©ì†Œë¦¬ë¡œ ìƒì„±í•˜ê³  ê³µë°±ì„ ì‚½ì…í•˜ì—¬ í•©ì¹©ë‹ˆë‹¤.

## ì‚¬ìš©ë²•
python generate_roleplay_audio.py
"""

import os
import asyncio
import wave
import subprocess
from pathlib import Path
import struct

from google import genai
from google.genai import types

# ì˜¤ë””ì˜¤ ì„¤ì •
RECEIVE_SAMPLE_RATE = 24000
CHANNELS = 1
SAMPLE_WIDTH = 2
SILENCE_DURATION = 3.0  # ëŒ€ì‚¬ ì‚¬ì´ ê³µë°± (ì´ˆ)

# ëª¨ë¸ ì„¤ì •
MODEL = "models/gemini-2.5-flash-native-audio-preview-12-2025"

# API í´ë¼ì´ì–¸íŠ¸
client = genai.Client(
    http_options={"api_version": "v1beta"},
    api_key=os.environ.get("GEMINI_API_KEY"),
)

# ì—­í• ê·¹ ëŒ€ì‚¬ (AëŠ” James - ë‚¨ì„±, BëŠ” Yuna - ì—¬ì„±)
ROLEPLAY_LINES = [
    ("A", "Hi! Nice to meet you. My name is James."),
    ("B", "Nice to meet you too, James. I'm Yuna. What do you do?"),
    ("A", "I'm a software engineer. I work at a tech company. How about you?"),
    ("B", "I'm a teacher. I teach at an elementary school."),
    ("A", "That's great! Where are you from?"),
    ("B", "I'm from Korea. And you?"),
    ("A", "I'm from the United States, but I live in Korea now."),
    ("B", "Wonderful! It was nice talking to you."),
    ("A", "You too! See you later!"),
]

# í™”ìë³„ ìŒì„± ì„¤ì •
VOICE_CONFIG_A = types.LiveConnectConfig(
    response_modalities=["AUDIO"],
    speech_config=types.SpeechConfig(
        voice_config=types.VoiceConfig(
            prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name="Puck")  # ë‚¨ì„± ìŒì„±
        )
    ),
)

VOICE_CONFIG_B = types.LiveConnectConfig(
    response_modalities=["AUDIO"],
    speech_config=types.SpeechConfig(
        voice_config=types.VoiceConfig(
            prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name="Kore")  # ì—¬ì„± ìŒì„±
        )
    ),
)


async def generate_line(speaker: str, text: str, output_path: str) -> bool:
    """ëŒ€ì‚¬ë¥¼ ì˜¤ë””ì˜¤ë¡œ ë³€í™˜í•˜ì—¬ WAV ì €ì¥"""
    config = VOICE_CONFIG_A if speaker == "A" else VOICE_CONFIG_B
    voice_name = "Puck (James)" if speaker == "A" else "Kore (Yuna)"
    print(f"  ğŸ¤ [{speaker}] {voice_name}: {text[:35]}...")
    
    audio_chunks = []
    
    try:
        async with client.aio.live.connect(model=MODEL, config=config) as session:
            await session.send(
                input=f"Read this line naturally in a friendly conversational tone: {text}",
                end_of_turn=True
            )
            
            turn = session.receive()
            async for response in turn:
                if data := response.data:
                    audio_chunks.append(data)
        
        if not audio_chunks:
            print(f"    âŒ ì˜¤ë””ì˜¤ ë°ì´í„° ì—†ìŒ")
            return False
        
        pcm_data = b"".join(audio_chunks)
        with wave.open(output_path, "wb") as wav_file:
            wav_file.setnchannels(CHANNELS)
            wav_file.setsampwidth(SAMPLE_WIDTH)
            wav_file.setframerate(RECEIVE_SAMPLE_RATE)
            wav_file.writeframes(pcm_data)
        
        return True
    except Exception as e:
        print(f"    âŒ ì˜¤ë¥˜: {e}")
        return False


def create_silence_wav(duration: float, output_path: str):
    """ì§€ì •ëœ ê¸¸ì´ì˜ ë¬´ìŒ WAV íŒŒì¼ ìƒì„±"""
    num_samples = int(RECEIVE_SAMPLE_RATE * duration)
    silence_data = struct.pack('<' + 'h' * num_samples, *([0] * num_samples))
    
    with wave.open(output_path, "wb") as wav_file:
        wav_file.setnchannels(CHANNELS)
        wav_file.setsampwidth(SAMPLE_WIDTH)
        wav_file.setframerate(RECEIVE_SAMPLE_RATE)
        wav_file.writeframes(silence_data)


def combine_wav_files(wav_files: list, silence_path: str, output_path: str):
    """WAV íŒŒì¼ë“¤ì„ ê³µë°±ê³¼ í•¨ê»˜ í•©ì¹˜ê¸°"""
    print(f"\nğŸ“¦ ì˜¤ë””ì˜¤ íŒŒì¼ í•©ì¹˜ëŠ” ì¤‘...")
    
    combined_data = b""
    
    for i, wav_path in enumerate(wav_files):
        with wave.open(wav_path, "rb") as wav_file:
            combined_data += wav_file.readframes(wav_file.getnframes())
        
        if i < len(wav_files) - 1:
            with wave.open(silence_path, "rb") as silence_file:
                combined_data += silence_file.readframes(silence_file.getnframes())
    
    with wave.open(output_path, "wb") as output_wav:
        output_wav.setnchannels(CHANNELS)
        output_wav.setsampwidth(SAMPLE_WIDTH)
        output_wav.setframerate(RECEIVE_SAMPLE_RATE)
        output_wav.writeframes(combined_data)
    
    print(f"  âœ… WAV ì €ì¥ ì™„ë£Œ: {output_path}")


def convert_to_mp3(wav_path: str, mp3_path: str) -> bool:
    """WAVë¥¼ MP3ë¡œ ë³€í™˜"""
    try:
        subprocess.run([
            "ffmpeg", "-y", "-i", wav_path,
            "-codec:a", "libmp3lame", "-qscale:a", "2",
            mp3_path
        ], check=True, capture_output=True)
        print(f"  âœ… MP3 ë³€í™˜ ì™„ë£Œ: {mp3_path}")
        return True
    except subprocess.CalledProcessError as e:
        print(f"  âš ï¸ MP3 ë³€í™˜ ì‹¤íŒ¨: {e.stderr.decode()}")
        return False
    except FileNotFoundError:
        print("  âš ï¸ ffmpegê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return False


async def main():
    output_dir = Path("docs/assets/audio")
    temp_dir = output_dir / "temp_roleplay"
    output_dir.mkdir(parents=True, exist_ok=True)
    temp_dir.mkdir(parents=True, exist_ok=True)
    
    print("=" * 60)
    print("ğŸ­ Day 4 Role Play ì˜¤ë””ì˜¤ ìƒì„±")
    print("=" * 60)
    print(f"ğŸ“ ì´ {len(ROLEPLAY_LINES)}ê°œ ëŒ€ì‚¬")
    print(f"ğŸ¤ A (James): Puck ìŒì„±")
    print(f"ğŸ¤ B (Yuna): Kore ìŒì„±")
    print(f"â±ï¸ ëŒ€ì‚¬ ì‚¬ì´ ê³µë°±: {SILENCE_DURATION}ì´ˆ")
    print("=" * 60)
    
    wav_files = []
    for i, (speaker, line) in enumerate(ROLEPLAY_LINES, 1):
        wav_path = temp_dir / f"line_{i:02d}.wav"
        success = await generate_line(speaker, line, str(wav_path))
        if success:
            wav_files.append(str(wav_path))
            print(f"    âœ… ëŒ€ì‚¬ {i}/{len(ROLEPLAY_LINES)} ì™„ë£Œ")
        else:
            print(f"    âŒ ëŒ€ì‚¬ {i}/{len(ROLEPLAY_LINES)} ì‹¤íŒ¨")
        
        await asyncio.sleep(1)
    
    if len(wav_files) != len(ROLEPLAY_LINES):
        print(f"\nâš ï¸ ì¼ë¶€ ëŒ€ì‚¬ ìƒì„± ì‹¤íŒ¨. {len(wav_files)}/{len(ROLEPLAY_LINES)} ì™„ë£Œ")
    
    silence_path = temp_dir / "silence.wav"
    create_silence_wav(SILENCE_DURATION, str(silence_path))
    print(f"\nğŸ”‡ ê³µë°± íŒŒì¼ ìƒì„±: {SILENCE_DURATION}ì´ˆ")
    
    combined_wav = output_dir / "week1_day4_roleplay.wav"
    combine_wav_files(wav_files, str(silence_path), str(combined_wav))
    
    final_mp3 = output_dir / "week1_day4_roleplay.mp3"
    if convert_to_mp3(str(combined_wav), str(final_mp3)):
        import shutil
        shutil.rmtree(temp_dir)
        os.remove(combined_wav)
        print("\nğŸ§¹ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")
    
    print("\n" + "=" * 60)
    print("âœ… Role Play ì˜¤ë””ì˜¤ ìƒì„± ì™„ë£Œ!")
    print(f"ğŸ“ íŒŒì¼ ìœ„ì¹˜: {final_mp3}")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())
